#!/usr/bin/env python3

from functools import reduce
from pybedtools import BedTool
import csv
import subprocess
import vcf
import re
import pandas as pd
import shlex

"""
This script can incorporate as many QC checks as required
and is adapted from the connor-lab pipeline located at
https://github.com/connor-lab/ncov2019-artic-nf
"""

def get_num_reads(bamfile):

    st_filter = '0x900'
    command = 'samtools view -c -F{} {}'.format(st_filter, bamfile)
    what = shlex.split(command)

    return subprocess.check_output(what).decode().strip()

def get_vcf_variants(variants_vcf, variants_list=[], locations=[]):
    '''
    Use pipeline pass.vcf.gz file to get passing variants and their locations
    INPUTS:
        variants_vcf   --> `path` from argparse to the input vcf.gz file
        variants_list  --> `list` to append the variants formated as RefPosAlt
                        ['G231T', 'A3423C']
        locations      --> `list` to append only the variant location ints to
    RETURNS:
        'None' or str variants separated by a ;
        location list or None for the variant int location
    '''
    vcf_reader = vcf.Reader(open(variants_vcf, 'rb'))
    for rec in vcf_reader:
        variant = '{}{}{}'.format(rec.REF, rec.POS, rec.ALT[0])

        # Checking for duplicate variants that have been an issue
        if variant in variants_list:
            pass
        else:
            variants_list.append(variant)
            locations.append(rec.POS)


    variants = (';'.join(variants_list))

    if variants == '':
        return 'None', None

    return variants, locations

def get_tsv_variants(variants_tsv, variants_list=[], locations=[]):
    '''
    Use pipeline variants.tsv file to get passing variants and their locations
    INPUTS:
        variants_tsv   --> `path` from argparse to the input tsv file generated by signal
        variants_list  --> `list` to append the variants formated as RefPosAlt
                        ['G231T', 'A3423C']
        locations      --> `list` to append only the variant location ints to
    RETURNS:
        `str` of 'None' or variants separated by ;
        `list` of locations or None if no variants found
    '''
    with open(variants_tsv) as input_handle:

        for index, line in enumerate(input_handle):

            # Pass the header line so as to not include it!
            if index == 0:
                continue

            row = line.strip('\n').split('\t') # Order is [REGION, POS, REF, ALT, ...]

            variant = '{}{}{}'.format(row[2], row[1], row[3])

            # Checking for duplicate variants that have been an issue
            if variant in variants_list:
                pass
            else:
                variants_list.append(variant)
                locations.append(row[1])
    
    variants = (';'.join(variants_list))

    if variants == '':
        return 'None', None
        
    return variants, locations

def find_primer_mutations(pcr_bed, genomic_locations, primer_mutations=[]):
    '''
    Use variant info to check for mutations in currently used primers
    INPUTS:
        pcr_bed            --> `path` from argparse to input bed file of PCR primers
        genomic_locations  --> `list` of integer locations to check or `None` if none found
        primer_mutations   --> `list` to append any primer mutations found
    RETURNS:
        `str` primer mutations statement
    '''
    if genomic_locations is None:
        return 'None'
    
    input_bed = BedTool(pcr_bed)

    for gene in input_bed:
        location = range(gene.start, gene.stop + 1) # Plus one to make sure that we get mutations in the final location of the range

        for variant_pos in genomic_locations:
            if variant_pos in location:
                primer_mutations.append('Variant position {} overlaps PCR primer {}'.format(variant_pos, gene.name))
    
    if primer_mutations != []:
        statement = '; '.join(primer_mutations)
        return 'Warning: {}'.format(statement)

    return 'None'

def get_lineage(pangolin_csv, sample_name):
    '''
    Check Pangolin output for the lineage of the sample
    INPUTS:
        pangolin_csv --> `path` from argparse to input pangolin csv file
        sample_name  --> `str` sample name from argparse
    RETURNS:
        `str` lineage
        `str` pango-designation version
    '''
    df = pd.read_csv(pangolin_csv)
    df_slice = df.loc[df['taxon'] == sample_name]

    if not df_slice.empty:
        lineage = df_slice['lineage'].item()
        pangoV = df_slice['version'].item()

        return lineage, pangoV

    else:
        return 'Unknown', 'Unknown'

def get_protein_variants(aa_table):
    '''
    Parse ncov-tools output to report its amino acid mutations
    INPUTS:
        aa_table  --> `path` from argparse to input aa_table.csv
    RETURNS:
        `str` amino acid mutations separated by a ;
    '''
    df = pd.read_csv(aa_table, sep='\t').dropna()
    return ';'.join(df['aa'].tolist())

def parse_ncov_tsv(file_in, sample, negative=False):
    '''
    Parse ncov-tools output tsv files (summary and negative) to grab data for the sample
    INPUTS:
        file_in   --> `path` from argparse to input ncov-tools tsv file to parse
        sample    --> `str` sample name from argparse
        negative  --> `boolean` for if the tsv table is for the negative control or not
    RETURNS:
        Populated `df`
    '''
    # Try to read file (as negative control may not have data in it)
    try:
        df = pd.read_csv(file_in, sep='\t')

    # If no data, we set up how it should be and then pass it through
    # Could also make is such that runs without negative ctrls just don't have the columns
    except pd.errors.EmptyDataError:
        negative_df = pd.DataFrame(columns=['negative_control_sample', 'negative_control_qc', 'negative_control_genome_covered_bases', 'negative_control_genome_total_bases', 'negative_control_genome_covered_fraction', 'negative_control_amplicons_detected'])
        negative_df.loc[1, 'sample'] = sample
        negative_df.fillna('NA', inplace=True)
    
        return negative_df

    # If the column headers get changed in the input just replace the new sample column below
    # First column is the file name
    if negative:
        new_columns = df.columns.values
        new_columns[0] = 'sample'
        # Rename neg control columns
        for spot in range(1, len(new_columns)):
            new_columns[spot] = 'negative_control_{}'.format(new_columns[spot])
        df.columns = new_columns

    # Input is summary_df, drop its lineage column as we pull and create our own (as it didn't have this before)
    # Run name is also unneeded
    else:
        df.drop(columns=['lineage', 'run_name'], inplace=True)

    # Set which column contains the sample
    sample_column = 'sample'

    # Finding the data, all samples will be in ncov-tools summary output (as they had data generated)
    for index, name in enumerate(df[sample_column].tolist()):
        if re.search(sample, name):
            df.loc[index, sample_column] = sample
            df.fillna('NA', inplace=True)
            return df.iloc[[index]]
    
    # If nothing is found, input is not a negative control and we need to keep negative columns
    negative_df = pd.DataFrame(columns=new_columns)
    negative_df.loc[1, sample_column] = sample
    negative_df.fillna('NA', inplace=True)
    
    return negative_df

def get_samplesheet_info(sample_tsv, sample_name):
    '''
    Parse samplesheet info to allow for IRIDA uploads and adding whatever data wanted to output qc file
    INPUTS:
        sample_tsv   --> `path` from argparse to input samplesheet.tsv file
        sample_name  --> `str` sample name from argparse
    RETURNS:
        `df` populated with data from samplesheet
    '''
    df = pd.read_csv(sample_tsv, sep='\t', dtype=object)
    # Rename run to run_identifier as that is what is already in IRIDA
    df.rename(columns={'run': 'run_identifier'}, inplace=True)
    samplesheet_columns = df.columns.values

    # ncov-tools or this script captures these columns from metadata file already, don't double dip them
    columns_to_remove = set(['ct', 'date', 'scheme', 'primer_scheme']).intersection(samplesheet_columns)
    if columns_to_remove:
        df.drop(columns=columns_to_remove, inplace=True)

    
    # Get only the sample row and if empty, fill it in to match other rows
    df = df.loc[df['sample'] == sample_name]
    if df.empty:
        df.loc[1, 'sample']  = sample_name

    df.fillna('NA', inplace=True)

    return df

def go(args):

    ## Number of aligned reads calculaton
    num_reads = get_num_reads(args.bam)

    ### Added checks ###
    ####################
    if args.nanopore:
        # Vcf passing variants from nanopore pipeline
        variants, variant_locations = get_vcf_variants(args.vcf)
    
    elif args.illumina:
        # Tsv variants from Illumina pipeline
        if args.tsv_variants:
            variants, variant_locations = get_tsv_variants(args.tsv_variants)
        # VCF variants from FREEBAYES
        else:
            variants, variant_locations = get_vcf_variants(args.vcf)

    # Find any overlap of variants in the pcr primer regions
    primer_statement = find_primer_mutations(args.pcr_bed, variant_locations)

    # Pangolin Lineages
    lineage, pango_designation_v = get_lineage(args.pangolin, args.sample)

    # snpEFF output
    protein_variants = get_protein_variants(args.snpeff_tsv)

    # NCOV-Tools Results
    summary_df = parse_ncov_tsv(args.ncov_summary, args.sample)
    negative_df = parse_ncov_tsv(args.ncov_negative, args.sample, negative=True)

    # If we have a samplesheet, use its values to create final output
    if args.sample_sheet:
        sample_sheet_df = get_samplesheet_info(args.sample_sheet, args.sample)

        qc_line = {  'sample' : [args.sample],
           'num_aligned_reads': [num_reads],
                    'lineage' : [lineage],
                   'variants' : [variants],
            'protein_variants': [protein_variants],
'diagnostic_primer_mutations' : [primer_statement],
                     'scheme' : [args.scheme],
      'sequencing_technology' : [args.sequencing_technology],
                    'version' : [pango_designation_v],
                'script_name' : [args.script_name],
                   'revision' : [args.revision]}
        
        qc_df = pd.DataFrame.from_dict(qc_line)
        data_frames = [sample_sheet_df, qc_df, summary_df, negative_df]

    else:
        qc_line = {      'sample' : [args.sample],
               'num_aligned_reads': [num_reads],
                        'lineage' : [lineage],
                       'variants' : [variants],
                'protein_variants': [protein_variants],
    'diagnostic_primer_mutations' : [primer_statement],
                         'scheme' : [args.scheme],
          'sequencing_technology' : [args.sequencing_technology],
                        'version' : [pango_designation_v],
                    'script_name' : [args.script_name],
                       'revision' : [args.revision]}

        qc_df = pd.DataFrame.from_dict(qc_line)

        data_frames = [qc_df, summary_df, negative_df]

    # Merge all dataframes together
    out_df = reduce(lambda left,right: pd.merge(left,right,on='sample', how='left'), data_frames)

    # Remove comma's as some of the ncov-tools fields have commas
    out_df.replace(',',';', regex=True, inplace=True)

    # Output
    out_df.to_csv(args.outfile, sep=',', index=False)

def main():
    import argparse

    parser = argparse.ArgumentParser()
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--nanopore', action='store_true')
    group.add_argument('--illumina', action='store_true')
    parser.add_argument('--outfile', required=True)
    parser.add_argument('--sample', required=True)
    parser.add_argument('--ref', required=True)
    parser.add_argument('--bam', required=True)
    parser.add_argument('--fasta', required=True)
    parser.add_argument('--pangolin', required=True)
    parser.add_argument('--ncov_summary', required=True)
    parser.add_argument('--ncov_negative', required=True)
    parser.add_argument('--revision', required=True)
    parser.add_argument('--script_name', required=True)
    parser.add_argument('--vcf', required=False)
    parser.add_argument('--tsv_variants', required=False)
    parser.add_argument('--sequencing_technology', required=True)
    parser.add_argument('--scheme', required=True)
    parser.add_argument('--snpeff_tsv', required=True)
    parser.add_argument('--pcr_bed', required=True)
    parser.add_argument('--sample_sheet', required=False)

    args = parser.parse_args()
    go(args)

if __name__ == "__main__":
    main()
